from typing import Callable
import torch
import torch.nn.functional as F

class FlowMatching:
    def __init__(self,time_scaler : Callable[[torch.Tensor],torch.Tensor] = None,eps=1e-2):
        self.eps = eps
        if time_scaler is None:
            time_scaler=lambda x:1
        self.time_scaler=time_scaler
    
    def flow_matching_pair(self,model,input_domain,target_domain, time = None):
        """
        Generates direction pairs for flow matching model training
        
        Parameters:
            model: 
                model(xt,t) -> direction prediction. 
                
                Takes linear combination of `input_domain` and `target_domain`
                
                `xt=(1-t)*input_domain+t*target_domain`
                
                `time` is vector of size `[BATCH]` in range `[0;1]`

            input_domain: 
                simple domain (standard normal noise)
                
            target_domain: 
                complex domain (images,time series, etc)
            
            time:
                time to sample inputs. If None, the random time in range [0;1] is generated
        
        Returns:
            Tuple[Tensor,Tensor]:
            1. Predicted direction
            2. Ground truth direction
            3. Time
        """
        # generate time in range [0;1]
        if time is None:
            time = torch.rand(input_domain.shape[0],device=input_domain.device)
        
        time_expand = time[:,*([None]*(target_domain.dim()-1))]
        xt = (1-time_expand)*input_domain+time_expand*target_domain
        
        pred_direction = model(xt,time)
        
        #original
        target = (target_domain-input_domain)*self.time_scaler(time_expand)
        
        return pred_direction,target, time_expand
    def contrastive_flow_matching_pair(self, model, input_domain, target_domain, time=None):
        """
        Generates flow matching training pairs along with contrastive pairs for 
        Contrastive Flow Matching (CFM).

        This extends standard flow matching by returning an additional "negative"
        direction vector. That vector can be used in a contrastive loss as 
        proposed in the paper *Contrastive Flow Matching (ΔFM)*.

        The method constructs interpolated states between `input_domain` and 
        `target_domain` at sampled times, computes the model’s prediction, 
        and provides both ground truth and negative direction vectors.

        Parameters
        ----------
        model : Callable
            Function or neural network of signature `model(x_t, t) -> direction_pred`.
            It predicts the flow direction given interpolated samples `x_t` and time `t`.

        input_domain : torch.Tensor
            Tensor representing the "simple" domain (e.g., standard Gaussian noise).
            Shape: `[B, ...]`

        target_domain : torch.Tensor
            Tensor representing the "complex" domain (e.g., images, time series).
            Shape: `[B, ...]`, same as `input_domain`.

        time : torch.Tensor, optional
            Tensor of shape `[B]` with values in `[0, 1]` representing interpolation 
            times. If None, random times are sampled uniformly.

        Returns
        -------
        pred_direction : torch.Tensor
            Model-predicted flow direction at interpolated state `x_t`.

        target_direction : torch.Tensor
            Ground truth flow direction (from `input_domain` to `target_domain`), 
            scaled by the time-dependent factor.

        contrastive_direction : torch.Tensor
            Ground truth direction vector sampled from a *different* element in the batch.
            This serves as the negative direction for contrastive loss.

        time_expand : torch.Tensor
            Time tensor broadcasted to match input dimensions for interpolation.

        Notes
        -----
        - Contrastive directions are generated by randomly shuffling the batch and 
        recomputing ground truth flow directions.
        - Loss should be computed externally, for example:

        >>> lambda_cf=0.05
        >>> mse_loss = F.mse_loss(pred_direction, target_direction)
        >>> contrastive_loss = F.mse_loss(pred_direction, contrastive_direction)
        >>> loss = mse_loss - lambda_cf * contrastive_loss
        """
        if time is None:
            time = torch.rand(input_domain.shape[0], device=input_domain.device)
        bsz = input_domain.shape[0]
        time_expand = time[:, *([None] * (target_domain.dim() - 1))]
        xt = (1 - time_expand) * input_domain + time_expand * target_domain

        pred_direction = model(xt, time)

        target = (target_domain - input_domain) * self.time_scaler(time_expand)

        # Prepare negative samples by shuffling the batch
        idx = torch.randperm(bsz, device=input_domain.device)
        input_neg = input_domain[idx]
        target_neg = target_domain[idx]
        time_expand_neg = time_expand[idx]

        target_neg_vec = (target_neg - input_neg) * self.time_scaler(time_expand_neg)

        return pred_direction, target, target_neg_vec, time_expand

    def sample(self,model, x0, steps, churn_scale=0.0, inverse=False,return_intermediates = False,device='cpu'):
        """
        Samples from a flow-matching model with Euler integration.

        Args:
            model: Callable vθ(x, t) predicting vector field/motion.
            x0: Starting point (image or noise tensor).
            steps: Number of Euler steps.
            churn_scale: Amount of noise added for stability each step.
            inverse (bool): If False, integrate forward from x0 to x1 (image → noise).
                            If True, reverse for noise → image.
            return_intermediates: to return intermediates values of xt or not.
        Returns:
            Tuple[Tensor,List[Tensor]]:
            1. xt - Final sample tensor.
            2. intermediates - Intermediate xt values if return_intermediates is True
        """
        if inverse: 
            ts = torch.linspace(1, 0, steps+1, device=device)
        else:
            ts = torch.linspace(0, 1, steps+1, device=device)

        x0 = x0.to(device)
        xt = x0
        dt = -1/steps if inverse else 1/steps
        
        intermediates = []
        with torch.no_grad():
            for i in range(0,steps):
                t = ts[i]
                
                # optional churn noise
                if churn_scale>0:
                    noise = xt.std() * torch.randn_like(xt) + xt.mean()
                    xt = churn_scale * noise + (1 - churn_scale) * xt
                
                t_expand = t.expand(x0.shape[0])
                
                t_scaler = self.time_scaler(t)+self.eps
                # original
                pred = model(xt, t_expand)/t_scaler
                
                # forward or reverse Euler update
                xt = xt + dt * pred
                if return_intermediates:
                    intermediates.append(xt)
        
        if return_intermediates:
            return xt, intermediates
        return xt
